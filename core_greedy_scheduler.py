import pandas as pd
from datetime import datetime, timedelta
import random
import sys
import ast
from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
from matplotlib.cm import get_cmap
from matplotlib.font_manager import FontProperties

# ============== å…¨åŸŸåŸºæœ¬åƒæ•¸ ==============
SETUP_TIME = 15
START_SCHEDULE_STR = "2025-07-20 00:00"   # æ™‚é–“ 0 åŸé»
LOCK_WINDOW = 2880        # 48 å°æ™‚ (åˆ†é˜) åªå° A é¡æ©Ÿå°æ›è£½ç¨‹é–å®š
INTER_STEP_GAP = 0
INFINITE_CAPACITY = 10**12  # æ•´ç†ç®±ç”¨
DEBUG = False
# =========================================

# ============== æª”æ¡ˆè·¯å¾‘ï¼ˆæŒ‰éœ€è¦ä¿®æ”¹ï¼‰ ==============
wipmq23_path = 'æ–°æ’ç¨‹0710.xlsx'
reference_path = 'find_time_T.xlsx'
time_T_path = 'time_T_update.xlsx'
machine_capacity_path = r"C:/Users/User/PycharmProjects/æ°¸å‹/0423/Machine ID Capacity.xlsx"
rule_path = r"C:/Users/User/PycharmProjects/æ°¸å‹/rule_0111æ›´æ–°æ›†æ…§ç‰ˆ.xlsx"
# ==============================================
# ====== å» å€å¯ç”¨æ©Ÿå°è¨­å®š ======
SITE_MACHINE_IDS = {
    '2A': [
        'P_1706','P_1708','P_1709','P_1710','P_1711',
        'P_6217','P_6218','P_6219','P_6220','P_6221',
        'SK12_2A01','SK12_2A02'
    ],
    '2B': [
        'P_1712','P_1713','P_1714','P_1720','P_1721','P_1722',
        'P_6222','P_6223','P_6224','P_6225','P_6226',
        'SK12_2A03','SK12_2A04','SK12_2A05'
    ]
}

_dup = set(SITE_MACHINE_IDS['2A']) & set(SITE_MACHINE_IDS['2B'])
if _dup:
    print("âš ï¸ è­¦å‘Šï¼š2A èˆ‡ 2B æ©Ÿå°é›†åˆæœ‰é‡è¤‡ï¼š", _dup)

# ===================================================================
# ä¸€ã€è³‡æ–™è®€å–èˆ‡æ•´ä½µ
# ===================================================================
def read_and_merge_data(
        keep_all_centers: bool = True,
        export_unmatched: bool = True,
        debug: bool = True
) -> str:
    cols = ["å·¥å–®ç·¨è™Ÿ", "æ–™è™Ÿ", "æ‰¹è™Ÿ", "å·¥å–®", "æ’éšŠæ•¸"]
    try:
        df_wip = pd.read_excel(wipmq23_path)[cols]
    except FileNotFoundError:
        sys.exit(f"âŒ æ‰¾ä¸åˆ° WIP æª”æ¡ˆï¼š{wipmq23_path}")
    if debug:
        print("[0] åŸå§‹ WIP åˆ—æ•¸ï¼š", len(df_wip))

    df_wip = df_wip.rename(columns={'æ–™è™Ÿ': 'æ–™å“ç·¨è™Ÿ'})
    df_wip.insert(df_wip.columns.get_loc("æ–™å“ç·¨è™Ÿ") + 1, "ç”¢å“ç·¨è™Ÿ",
                  df_wip["æ–™å“ç·¨è™Ÿ"].astype(str).str[:5])
    df_wip["æ‰¹è™Ÿ"] = df_wip["æ‰¹è™Ÿ"].astype(str)
    df_wip.insert(df_wip.columns.get_loc("æ‰¹è™Ÿ") + 1,
                  "å·¥å–®åˆ¥", df_wip["æ‰¹è™Ÿ"].str[:1].fillna(''))

    if debug:
        print("[2] ç”¢å“ç·¨è™Ÿç¼ºå¤±ï¼š", df_wip["ç”¢å“ç·¨è™Ÿ"].isna().sum(),
              "å·¥å–®åˆ¥ç©ºå­—ä¸²ï¼š", df_wip["å·¥å–®åˆ¥"].eq('').sum())

    try:
        xls = pd.ExcelFile(reference_path)
        df_ref = pd.read_excel(xls, sheet_name="æ–™è™Ÿæ°´åˆå°æ‡‰è¡¨")
        df_product = pd.read_excel(xls, sheet_name="ç”¢å“æ°´åˆå°æ‡‰è¡¨")
    except FileNotFoundError:
        sys.exit(f"âŒ æ‰¾ä¸åˆ°åƒè€ƒæª”ï¼š{reference_path}")

    df_ref["ç”¢å“ç·¨è™Ÿ"] = df_ref["æ¿•ç‰‡ç¾¤çµ„"].astype(str).str.zfill(5).str[:5]
    df_ref["å·¥å–®åˆ¥"] = df_ref["å·¥å–®åˆ¥"].astype(str).fillna('')
    product_mapping = df_product.set_index("ç”¢å“ç¢¼åˆ†é¡")["ç”¢å“"].to_dict()

    unique_ref = (
        df_ref[["ç”¢å“ç·¨è™Ÿ", "å·¥å–®åˆ¥", "ç”¢å“åˆ†é¡", "æ°´åˆä»£è™Ÿ"]]
        .drop_duplicates()
        .assign(ç”¢å“=lambda d: d["ç”¢å“åˆ†é¡"].map(product_mapping))
    )

    final_data = df_wip.merge(unique_ref,
                              on=["ç”¢å“ç·¨è™Ÿ", "å·¥å–®åˆ¥"],
                              how="left",
                              indicator=True)
    if debug:
        print("[3] merge çµæœï¼š\n", final_data["_merge"].value_counts())

    if export_unmatched:
        unmatched = final_data[final_data["_merge"] == "left_only"]
        if not unmatched.empty:
            unmatched_file = "unmatched_wip.xlsx"
            unmatched.to_excel(unmatched_file, index=False)
            print(f"âš ï¸ æœ‰ {len(unmatched)} ç­† WIP ç„¡å°æ‡‰åƒè€ƒè¡¨ï¼Œå·²è¼¸å‡º {unmatched_file}")

    final_data.drop(columns="_merge", inplace=True)

    try:
        time_T = pd.read_excel(time_T_path, sheet_name="å·¥ä½œè¡¨1").ffill()
    except FileNotFoundError:
        sys.exit(f"âŒ æ‰¾ä¸åˆ°å·¥æ™‚æª”ï¼š{time_T_path}")

    time_columns = [c for c in time_T.columns if c not in ["ç”¢å“åˆ†é¡", "æ°´åˆä»£è™Ÿ"]]
    time_map = (
        time_T.set_index(["ç”¢å“åˆ†é¡", "æ°´åˆä»£è™Ÿ"])[time_columns]
        .to_dict(orient="index")
    )

    def fetch_times(row):
        return time_map.get(
            (row["ç”¢å“åˆ†é¡"], row["æ°´åˆä»£è™Ÿ"]),
            {c: None for c in time_columns}
        )

    times_df = final_data.apply(fetch_times, axis=1, result_type="expand")
    final_data[time_columns] = times_df

    if export_unmatched:
        missing_time = final_data[times_df.isna().all(axis=1)]
        if not missing_time.empty:
            missing_file = "missing_time_rows.xlsx"
            missing_time.to_excel(missing_file, index=False)
            print(f"âš ï¸ æœ‰ {len(missing_time)} ç­†ç¼ºå°‘å·¥æ™‚è³‡æ–™ï¼Œå·²è¼¸å‡º {missing_file}")

    output_file = "scheduling_result 20250718.xlsx"
    final_data.to_excel(output_file, index=False)
    print(f"âœ… æ•´åˆè³‡æ–™å·²è¼¸å‡ºï¼š{output_file}")
    return output_file

# ========== 2. æº–å‚™æ‰¹æ¬¡è³‡æ–™ï¼ˆè½‰æˆè£½ç¨‹æ­¥é©Ÿå±•é–‹ï¼‰==========
def prepare_batch_scheduling_data(scheduling_file):
    machine_df = pd.read_excel(machine_capacity_path)
    rule_df = pd.read_excel(rule_path, sheet_name='æ°´åˆ')
    scheduling_df = pd.read_excel(scheduling_file)

    # çµ¦éš¨æ©Ÿäº¤æœŸ (å¯æ›æˆçœŸå¯¦)
    base_date = datetime.strptime(START_SCHEDULE_STR, "%Y-%m-%d %H:%M")
    scheduling_df['äº¤æœŸ'] = [
        (base_date + timedelta(days=random.randint(1, 30))).strftime("%Y-%m-%d")
        for _ in range(len(scheduling_df))
    ]

    # å‡è¨­ç¬¬ 12~30 æ¬„æ˜¯è£½ç¨‹æ™‚é–“æ¬„ä½ï¼ˆä¾ä½ åŸå§‹ï¼‰
    process_cols = scheduling_df.columns[12:30]
    # ç§»é™¤å…¨ç©ºè£½ç¨‹åˆ—
    scheduling_df = scheduling_df[
        scheduling_df[process_cols].apply(
            lambda row: row.notna().any() and (row.astype(str).str.strip() != '').any(),
            axis=1
        )
    ].reset_index(drop=True)

    # ç”Ÿæˆè£½ç¨‹çµ„åˆèˆ‡çµ„åˆç·¨è™Ÿ
    # === å»ºç«‹è£½ç¨‹çµ„åˆï¼ˆtuple of (æ¬„å, floatå€¼)ï¼‰ ===
    def extract_process_tuple(row):
        combo = []
        for c in process_cols:
            v = row[c]
            try:
                if pd.notna(v) and str(v).strip() != '' and float(v) > 0:
                    combo.append((c, float(v)))
            except:
                continue
        return tuple(combo)  # å¯èƒ½ç‚ºç©º tuple()

    scheduling_df['è£½ç¨‹çµ„åˆ'] = scheduling_df.apply(extract_process_tuple, axis=1)

    # === å»æ‰å®Œå…¨ç©ºçš„çµ„åˆï¼ˆå¦‚æœå¸Œæœ›ä¿ç•™å¯åˆªæ‰é€™æ®µï¼‰===
    empty_cnt = (scheduling_df['è£½ç¨‹çµ„åˆ'].map(len) == 0).sum()
    if empty_cnt > 0:
        print(f"[prepare] ç§»é™¤ç©ºè£½ç¨‹çµ„åˆåˆ— {empty_cnt} ç­†")
        scheduling_df = scheduling_df[scheduling_df['è£½ç¨‹çµ„åˆ'].map(len) > 0].reset_index(drop=True)

    # === è½‰æˆæ¨™æº–å­—ä¸²ï¼ˆé¿å… tuple é•·åº¦ä¸åŒçš„å•é¡Œ / ä¹Ÿæ–¹ä¾¿æ¯”å°ï¼‰===
    def combo_to_str(tup):
        # tup å½¢å¼: ((ProcCol, duration), ...)
        return '|'.join(f"{p}:{d:g}" for p, d in tup)

    scheduling_df['è£½ç¨‹çµ„åˆ_str'] = scheduling_df['è£½ç¨‹çµ„åˆ'].apply(combo_to_str)

    # === ç”¨ factorize ç”¢ç”Ÿçµ„åˆç·¨è™Ÿï¼ˆå¾ 1 èµ·ç®—ï¼‰===
    codes, uniques = pd.factorize(scheduling_df['è£½ç¨‹çµ„åˆ_str'], sort=False)
    scheduling_df['çµ„åˆç·¨è™Ÿ'] = codes + 1

    print(f"[prepare] ä¸åŒè£½ç¨‹çµ„åˆæ•¸é‡ï¼š{len(uniques)}")
    if len(uniques) <= 10:
        print("[prepare] å‰å¹¾å€‹çµ„åˆç¤ºä¾‹ï¼š")
        for i, u in enumerate(uniques[:10], 1):
            print(f"  çµ„åˆ {i}: {u}")
    scheduling_df.to_excel(scheduling_file, index=False)
    return scheduling_df, machine_df, rule_df

# =============== æ ¸å¿ƒï¼šå»º job è¡¨ ===============

def build_jobs_from_scheduling(scheduling_df):
    process_cols = scheduling_df.columns[12:29]  # ç¢ºèª slicing æ­£ç¢º
    records = []
    for _, row in scheduling_df.iterrows():
        wo = row['å·¥å–®ç·¨è™Ÿ']
        item = row['æ–™å“ç·¨è™Ÿ']
        qty = row['æ’éšŠæ•¸']
        due = row['äº¤æœŸ']
        combo = row['çµ„åˆç·¨è™Ÿ']
        seq = 1
        for pcol in process_cols:
            val = row.get(pcol)
            if pd.notna(val) and str(val).strip() != '':
                try:
                    ft = float(val)
                    if ft > 0:
                        records.append({
                            'å·¥å–®ç·¨è™Ÿ': wo,
                            'æ–™å“ç·¨è™Ÿ': item,
                            'æ’éšŠæ•¸': qty,
                            'äº¤æœŸ': due,
                            'çµ„åˆç·¨è™Ÿ': combo,
                            'è£½ç¨‹é †åº': seq,
                            'è£½ç¨‹åç¨±': pcol,
                            'è£½ç¨‹æ™‚é–“(åˆ†é˜)': ft
                        })
                        seq += 1
                except:
                    continue
    jobs_df = pd.DataFrame(records)
    if jobs_df.empty:
        raise ValueError("âš ï¸ ç„¡ä»»ä½•å¯æ’è£½ç¨‹åˆ—ã€‚")
    start_origin = datetime.strptime(START_SCHEDULE_STR, "%Y-%m-%d %H:%M")
    jobs_df['äº¤æœŸ_dt'] = pd.to_datetime(jobs_df['äº¤æœŸ'])
    jobs_df['äº¤æœŸ_ç›¸å°åˆ†'] = (jobs_df['äº¤æœŸ_dt'] - start_origin).dt.total_seconds() / 60.0
    jobs_df['äº¤æœŸ_ç›¸å°åˆ†'] = jobs_df['äº¤æœŸ_ç›¸å°åˆ†'].fillna(jobs_df['äº¤æœŸ_ç›¸å°åˆ†'].max() or 0)
    return jobs_df


# =============== æ©Ÿå°è³‡æ–™è™•ç† ===============

def build_machine_tables(machine_df, rule_df):
    # ä¾ index åˆ‡ A/B/C (èˆ‡ä½ åŸç¨‹å¼ä¸€è‡´ï¼Œå¦‚æœ‰ç•°å‹•éœ€èª¿æ•´)
    df_type1 = machine_df.iloc[0:16].copy(); df_type1['é¡åˆ¥'] = 'A'
    df_type2 = machine_df.iloc[17:27].copy(); df_type2['é¡åˆ¥'] = 'B'
    df_type3 = machine_df.iloc[28:34].copy(); df_type3['é¡åˆ¥'] = 'C'
    all_machines_df = pd.concat([df_type1, df_type2, df_type3], ignore_index=True)

    machine_liquid_restrictions = {}
    for _, row in rule_df.iterrows():
        m_id = row["Machine_ID"]
        allow = str(row.get("èªªæ˜", "")).split(",") if pd.notna(row.get("èªªæ˜")) else []
        machine_liquid_restrictions[m_id] = [a.strip() for a in allow if a.strip()]

    # æ‰‹å‹•è¦†å¯«
    mapping_override = {
        'P_1706': ['æ³Šæ´›æ²™å§†','é æ³¡ç´”æ°´','æ¨™æº–é¹½æ°´'],
        'P_1708': ['ç¢³é…¸éˆ‰'],
        'P_1709': ['ç¢³é…¸éˆ‰'],
        'P_1710': ['BPé¹½æ°´'],
        'P_1711': ['æ¨™æº–é¹½æ°´','ç´”æ°´'],
        'P_1712': ['ç¢³é…¸éˆ‰'],
        'P_1713': ['ç¢³é…¸éˆ‰'],
        'P_1714': ['ç¢³é…¸éˆ‰'],
        'P_1720': ['æ¨™æº–é¹½æ°´'],
        'P_1721': ['æ³Šæ´›æ²™å§†'],
        'P_1722': ['é æ³¡ç´”æ°´','ç´”æ°´'],
    }
    machine_liquid_restrictions.update(mapping_override)

    def ensure_list(x):
        if isinstance(x, list): return x
        if isinstance(x, str):
            try:
                obj = ast.literal_eval(x)
                if isinstance(obj, list):
                    return [str(z).strip() for z in obj]
            except:
                return [s.strip() for s in x.split(",") if s.strip()]
        return []

    all_machines_df['å¯ç”¨è£½ç¨‹åç¨±'] = (
        all_machines_df['Machine_ID']
        .map(machine_liquid_restrictions)
        .apply(ensure_list)
    )

    # ç´°é¡åˆ¥
    category_capacity_groups = defaultdict(list)
    for _, row in all_machines_df.iterrows():
        key = (row['é¡åˆ¥'], row['å®¹é‡'])
        category_capacity_groups[key].append(row['Machine_ID'])

    sub_map = {}
    for cat in ['A', 'B', 'C']:
        sub_id = 1
        for (main_cat, cap), mlst in sorted(category_capacity_groups.items()):
            if main_cat == cat:
                sub_name = f"{cat}{sub_id}"
                for mid in mlst:
                    sub_map[mid] = sub_name
                sub_id += 1
    all_machines_df['æ©Ÿå°ç´°é¡åˆ¥'] = all_machines_df['Machine_ID'].map(sub_map)
    return all_machines_df


# =============== åˆ†å» å€ (2A / 2B) ===============

def split_sites(jobs_df):
    def detect_prefix(wo):
        s = str(wo)
        if s.startswith("2A"): return "2A"
        if s.startswith("2B"): return "2B"
        return "2A"  # é è¨­
    jobs_df = jobs_df.copy()
    jobs_df['å» å€'] = jobs_df['å·¥å–®ç·¨è™Ÿ'].apply(detect_prefix)
    jobs_2A = jobs_df[jobs_df['å» å€'] == '2A'].reset_index(drop=True)
    jobs_2B = jobs_df[jobs_df['å» å€'] == '2B'].reset_index(drop=True)
    return jobs_2A, jobs_2B


# =============== æ©Ÿå° State ===============

class MachineState:
    def __init__(self, machine_id, main_cat, sub_cat, capacity, allowed_processes, is_dummy=False):
        self.machine_id = machine_id
        self.main_cat = main_cat
        self.sub_cat = sub_cat
        self.capacity = capacity
        self.allowed = set([p.strip() for p in allowed_processes]) if allowed_processes else set()
        self.is_dummy = is_dummy
        self.available_time = 0
        self.last_process = None
        self.last_change_time = -10**12
        self.batches = []


# =============== æ ¸å¿ƒæ’ç¨‹ï¼ˆå–®å» å€ï¼‰ ===============

def schedule_one_site(site_jobs_df, machines_df, site_label):
    if site_jobs_df.empty:
        return pd.DataFrame(), 0

    process_to_categories = defaultdict(set)
    for _, m in machines_df.iterrows():
        for proc in m['å¯ç”¨è£½ç¨‹åç¨±']:
            process_to_categories[proc.strip()].add(m['é¡åˆ¥'])

    def assign_main_category(proc):
        poss = process_to_categories.get(proc, set())
        for c in ['A', 'B', 'C']:
            if c in poss: return c
        return None

    jobs = site_jobs_df.copy()
    jobs['æ©Ÿå°ä¸»é¡åˆ¥'] = jobs['è£½ç¨‹åç¨±'].apply(assign_main_category)

    all_supported = set()
    for _, m in machines_df.iterrows():
        for proc in m['å¯ç”¨è£½ç¨‹åç¨±']:
            all_supported.add(proc.strip())

    # è‹¥è¦æ”¹æˆï¼šè‹¥ç„¡æ”¯æ´ç›´æ¥å ±éŒ¯ (ä¸é€²æ•´ç†ç®±)ï¼š
    # unsupported = ~jobs['è£½ç¨‹åç¨±'].isin(all_supported)
    # if unsupported.any():
    #     raise ValueError(f"{site_label} æœ‰è£½ç¨‹ç„¡æ©Ÿå°æ”¯æ´ï¼š{jobs.loc[unsupported,'è£½ç¨‹åç¨±'].unique()}")
    # jobs['æ˜¯å¦æ•´ç†ç®±'] = False

    jobs['æ˜¯å¦æ•´ç†ç®±'] = ~jobs['è£½ç¨‹åç¨±'].isin(all_supported)

    jobs = jobs.sort_values(['å·¥å–®ç·¨è™Ÿ', 'æ–™å“ç·¨è™Ÿ', 'è£½ç¨‹é †åº']).reset_index(drop=True)
    jobs['Job_ID'] = jobs.index
    predecessor = {}
    chain_map = defaultdict(list)
    for _, r in jobs.iterrows():
        chain_map[(r['å·¥å–®ç·¨è™Ÿ'], r['æ–™å“ç·¨è™Ÿ'])].append(r['Job_ID'])
    for chain_ids in chain_map.values():
        for i, jid in enumerate(chain_ids):
            predecessor[jid] = chain_ids[i-1] if i > 0 else None

    machine_states = []
    for _, m in machines_df.iterrows():
        machine_states.append(
            MachineState(
                machine_id=m['Machine_ID'],
                main_cat=m['é¡åˆ¥'],
                sub_cat=m['æ©Ÿå°ç´°é¡åˆ¥'],
                capacity=m['å®¹é‡'],
                allowed_processes=m['å¯ç”¨è£½ç¨‹åç¨±'],
                is_dummy=False
            )
        )

    dummy_machine = MachineState(
        machine_id="æ•´ç†ç®±",
        main_cat="æ•´ç†ç®±",
        sub_cat=None,
        capacity=INFINITE_CAPACITY,
        allowed_processes=None,
        is_dummy=True
    )

    unscheduled = set(jobs['Job_ID'])
    scheduled_info = {}

    def job_ready(jid):
        pred = predecessor[jid]
        if pred is None:
            return True
        return pred in scheduled_info

    def predecessor_finish_time(jid):
        pred = predecessor[jid]
        if pred is None:
            return 0
        return scheduled_info[pred]['end']

    current_time = 0
    batch_id_counter = 0

    while unscheduled:
        # å…ˆæ’æ•´ç†ç®± ready
        ready_dummy_jobs = [
            jid for jid in list(unscheduled)
            if jobs.loc[jid, 'æ˜¯å¦æ•´ç†ç®±'] and job_ready(jid)
        ]
        for jid in ready_dummy_jobs:
            row = jobs.loc[jid]
            start_t = predecessor_finish_time(jid)
            if start_t < current_time:
                start_t = current_time
            duration = row['è£½ç¨‹æ™‚é–“(åˆ†é˜)'] + SETUP_TIME
            end_t = start_t + duration
            scheduled_info[jid] = {
                'start': start_t,
                'end': end_t,
                'machine': dummy_machine.machine_id,
                'machine_cat': dummy_machine.main_cat,
                'machine_sub': dummy_machine.sub_cat,
                'process': row['è£½ç¨‹åç¨±'],
                'batch_id': f"D{batch_id_counter}",
                'batch_seq': 1
            }
            batch_id_counter += 1
            unscheduled.remove(jid)

        active_jobs = [
            jid for jid in unscheduled
            if not jobs.loc[jid, 'æ˜¯å¦æ•´ç†ç®±']
        ]
        if not active_jobs:
            if not unscheduled:
                break
            future_pred_finishes = []
            for jid in unscheduled:
                pred = predecessor[jid]
                if pred is not None and pred in scheduled_info:
                    future_pred_finishes.append(scheduled_info[pred]['end'])
            if future_pred_finishes:
                current_time = max(current_time, min(future_pred_finishes))
            else:
                current_time += 1
            continue

        candidate_batches = []
        ready_jobs = [jid for jid in active_jobs if job_ready(jid)]
        if not ready_jobs:
            future_times = []
            for jid in active_jobs:
                pred = predecessor[jid]
                if pred is not None and pred in scheduled_info:
                    future_times.append(scheduled_info[pred]['end'])
            if future_times:
                current_time = max(current_time, min(future_times))
            else:
                current_time += 1
            continue

        for m in machine_states:
            earliest_machine_time = max(current_time, m.available_time)
            feasible_jobs = []
            for jid in ready_jobs:
                proc = jobs.loc[jid, 'è£½ç¨‹åç¨±']
                if proc in m.allowed:
                    feasible_jobs.append(jid)

            if not feasible_jobs:
                continue

            proc_groups = defaultdict(list)
            for jid in feasible_jobs:
                p = jobs.loc[jid, 'è£½ç¨‹åç¨±']
                proc_groups[p].append(jid)

            for proc_name, jids in proc_groups.items():
                jids.sort(
                    key=lambda x: (
                        jobs.loc[x, 'äº¤æœŸ_ç›¸å°åˆ†'],
                        predecessor_finish_time(x),
                        jobs.loc[x, 'æ’éšŠæ•¸']
                    )
                )
                batch_list = []
                used_qty = 0
                for jid in jids:
                    qty = jobs.loc[jid, 'æ’éšŠæ•¸']
                    if used_qty + qty <= m.capacity:
                        batch_list.append(jid)
                        used_qty += qty
                    else:
                        continue
                    if used_qty == m.capacity:
                        break
                if not batch_list:
                    continue

                pred_ready_time = max(predecessor_finish_time(j) for j in batch_list)
                start_t = max(earliest_machine_time, pred_ready_time)
                if m.main_cat == 'A':
                    if m.last_process is not None and m.last_process != proc_name:
                        start_t = max(start_t, m.last_change_time + LOCK_WINDOW)

                durations = [
                    jobs.loc[j, 'è£½ç¨‹æ™‚é–“(åˆ†é˜)'] + SETUP_TIME
                    for j in batch_list
                ]
                batch_dur = max(durations)
                end_t = start_t + batch_dur
                candidate_batches.append({
                    'machine': m,
                    'jobs': batch_list,
                    'process': proc_name,
                    'start': start_t,
                    'end': end_t,
                    'avg_due': np.mean([jobs.loc[j, 'äº¤æœŸ_ç›¸å°åˆ†'] for j in batch_list])
                })

        if not candidate_batches:
            future_preds = []
            for jid in active_jobs:
                pred = predecessor[jid]
                if pred is not None and pred in scheduled_info:
                    future_preds.append(scheduled_info[pred]['end'])
            if future_preds:
                current_time = max(current_time, min(future_preds))
            else:
                current_time += 1
            continue

        candidate_batches.sort(
            key=lambda b: (b['end'], b['start'], b['avg_due'])
        )
        chosen = candidate_batches[0]
        m = chosen['machine']
        start_t = chosen['start']
        end_t = chosen['end']
        proc_name = chosen['process']
        job_ids = chosen['jobs']

        m.available_time = end_t
        if m.main_cat == 'A' and (m.last_process is None or m.last_process != proc_name):
            m.last_change_time = start_t
        m.last_process = proc_name
        m.batches.append((batch_id_counter, start_t, end_t, proc_name, job_ids))

        for seq_in_batch, jid in enumerate(job_ids, start=1):
            row = jobs.loc[jid]
            j_dur = row['è£½ç¨‹æ™‚é–“(åˆ†é˜)'] + SETUP_TIME
            indiv_end = start_t + j_dur
            scheduled_info[jid] = {
                'start': start_t,
                'end': indiv_end,
                'machine': m.machine_id,
                'machine_cat': m.main_cat,
                'machine_sub': m.sub_cat,
                'process': proc_name,
                'batch_id': f"B{batch_id_counter}",
                'batch_seq': seq_in_batch
            }
            unscheduled.remove(jid)
        batch_id_counter += 1

    sched_records = []
    for jid, info in scheduled_info.items():
        row = jobs.loc[jid]
        sched_records.append({
            'å» å€': row['å» å€'],
            'å·¥å–®ç·¨è™Ÿ': row['å·¥å–®ç·¨è™Ÿ'],
            'æ–™å“ç·¨è™Ÿ': row['æ–™å“ç·¨è™Ÿ'],
            'çµ„åˆç·¨è™Ÿ': row['çµ„åˆç·¨è™Ÿ'],
            'è£½ç¨‹é †åº': row['è£½ç¨‹é †åº'],
            'è£½ç¨‹åç¨±': row['è£½ç¨‹åç¨±'],
            'æ’éšŠæ•¸': row['æ’éšŠæ•¸'],
            'è£½ç¨‹æ™‚é–“(åˆ†é˜)': row['è£½ç¨‹æ™‚é–“(åˆ†é˜)'],
            'å¯¦éš›è™•ç†æ™‚é–“(å«SETUP)': row['è£½ç¨‹æ™‚é–“(åˆ†é˜)'] + SETUP_TIME,
            'äº¤æœŸ': row['äº¤æœŸ'],                # â† æ–°å¢
            'é–‹å§‹(åˆ†)': info['start'],
            'çµæŸ(åˆ†)': info['end'],
            'æ©Ÿå°': info['machine'],
            'æ©Ÿå°ä¸»é¡åˆ¥': info['machine_cat'],
            'æ©Ÿå°ç´°é¡åˆ¥': info['machine_sub'],
            'æ‰¹æ¬¡ID': info['batch_id'],
            'æ‰¹æ¬¡å…§åº': info['batch_seq'],
            'æ˜¯å¦æ•´ç†ç®±': (info['machine'] == 'æ•´ç†ç®±'),
            'Job_ID': jid
        })

    schedule_df = pd.DataFrame(sched_records)
    if schedule_df.empty:
        return schedule_df, 0

    schedule_df = schedule_df.sort_values(
        ['å·¥å–®ç·¨è™Ÿ', 'æ–™å“ç·¨è™Ÿ', 'è£½ç¨‹é †åº']
    ).reset_index(drop=True)
    schedule_df['Gap_prev'] = 0
    grp = schedule_df.groupby(['å·¥å–®ç·¨è™Ÿ', 'æ–™å“ç·¨è™Ÿ'])
    for (wo, item), g in grp:
        prev_end = None
        for idx2 in g.index:
            if prev_end is None:
                schedule_df.at[idx2, 'Gap_prev'] = 0
            else:
                gap = schedule_df.at[idx2, 'é–‹å§‹(åˆ†)'] - prev_end
                schedule_df.at[idx2, 'Gap_prev'] = gap
            prev_end = schedule_df.at[idx2, 'çµæŸ(åˆ†)']

    # æ–°å¢ä½ è¦çš„ä¸­æ–‡æ¬„ä½
    schedule_df['é–“éš”æ™‚é–“(åˆ†)'] = schedule_df['Gap_prev']

    makespan = schedule_df['çµæŸ(åˆ†)'].max()
    return schedule_df, makespan


# =============== ä¸»æ’ç¨‹ (å« 2A / 2B æ©Ÿå°éæ¿¾ + è¼¸å‡ºå¤šå·¥ä½œè¡¨) ===============

def schedule_all(scheduling_file):
    scheduling_df, machine_df, rule_df = prepare_batch_scheduling_data(scheduling_file)
    jobs_df = build_jobs_from_scheduling(scheduling_df)
    all_machines_df = build_machine_tables(machine_df, rule_df)

    # å» å€æ‹†åˆ†
    jobs_2A, jobs_2B = split_sites(jobs_df)

    # éæ¿¾å» å€å°ˆç”¨æ©Ÿå°
    def filter_site_machines(site_label):
        wanted = set(SITE_MACHINE_IDS.get(site_label, []))
        site_df = all_machines_df[all_machines_df['Machine_ID'].isin(wanted)].copy()
        missing = wanted - set(site_df['Machine_ID'])
        if missing:
            print(f"âš ï¸ {site_label} è¨­å®šæ©Ÿå°åœ¨ machine_df æ‰¾ä¸åˆ°ï¼š{missing}")
        if site_df.empty:
            print(f"âŒ {site_label} ç„¡æœ‰æ•ˆæ©Ÿå°ï¼Œè©²å» å€å…¨éƒ¨å·¥ä½œå°‡é€²æ•´ç†ç®±ï¼")
        return site_df

    machines_2A = filter_site_machines('2A')
    machines_2B = filter_site_machines('2B')

    overlap = set(machines_2A['Machine_ID']) & set(machines_2B['Machine_ID'])
    if overlap:
        print("âš ï¸ è­¦å‘Šï¼š2A / 2B å¯¦éš›æ©Ÿå°é›†åˆæœ‰é‡ç–Šï¼š", overlap)

    sched_2A, mk_2A = schedule_one_site(jobs_2A, machines_2A, "2A")
    sched_2B, mk_2B = schedule_one_site(jobs_2B, machines_2B, "2B")

    start_origin = datetime.strptime(START_SCHEDULE_STR, "%Y-%m-%d %H:%M")
    for df in (sched_2A, sched_2B):
        if not df.empty:
            df['é–‹å§‹æ™‚é–“'] = df['é–‹å§‹(åˆ†)'].apply(lambda m: start_origin + timedelta(minutes=m))
            df['çµæŸæ™‚é–“'] = df['çµæŸ(åˆ†)'].apply(lambda m: start_origin + timedelta(minutes=m))

    full_makespan = max(mk_2A, mk_2B)

    # ç”¢å‡º Excelï¼š2A / 2B åˆ† Sheet
    out_file = "æ‰¹æ¬¡æ’ç¨‹çµæœ_greedy.xlsx"
    with pd.ExcelWriter(out_file, engine='xlsxwriter') as writer:
        sched_2A.to_excel(writer, sheet_name='2A', index=False)
        sched_2B.to_excel(writer, sheet_name='2B', index=False)
        # å¯é¸ Summaryï¼š
        summary_df = pd.DataFrame([
            {'å» å€': '2A', 'Makespan(åˆ†)': mk_2A, 'ä»»å‹™æ•¸': len(sched_2A)},
            {'å» å€': '2B', 'Makespan(åˆ†)': mk_2B, 'ä»»å‹™æ•¸': len(sched_2B)},
            {'å» å€': 'Overall', 'Makespan(åˆ†)': full_makespan,
             'ä»»å‹™æ•¸': len(sched_2A) + len(sched_2B)}
        ])
        summary_df.to_excel(writer, sheet_name='Summary', index=False)

    print(f"âœ… æ’ç¨‹å®Œæˆï¼Œè¼¸å‡ºï¼š{out_file}")
    print(f"ğŸ‘‰ 2A makespan: {mk_2A} åˆ†, 2B makespan: {mk_2B} åˆ†, Overall makespan: {full_makespan} åˆ†")
    return sched_2A, sched_2B, full_makespan

def add_proc_labels(df):
    """
    ç‚º DataFrame åŠ ä¸Š:
    â”€ Proc_Serial : æ¯å€‹è£½ç¨‹åç¨±çš„æµæ°´è™Ÿï¼Œä¾é–‹å§‹æ™‚é–“éå¢
                    (åŒä¸€æ©Ÿå°ã€åŒä¸€é–‹å§‹æ™‚é–“å…±ç”¨åŒè™Ÿ)
    â”€ Batch_Label : è£½ç¨‹åç¨±-æµæ°´è™Ÿï¼Œç”¨æ–¼ç”˜ç‰¹åœ–æ¨™ç±¤
    """
    if df.empty:
        return df

    df = df.copy()
    # å…ˆä¾ è£½ç¨‹åç¨± â†’ é–‹å§‹(åˆ†) â†’ æ©Ÿå° åšç©©å®šæ’åº
    df.sort_values(['è£½ç¨‹åç¨±', 'é–‹å§‹(åˆ†)', 'æ©Ÿå°'], inplace=True)

    # key = (è£½ç¨‹åç¨±, æ©Ÿå°, é–‹å§‹æ™‚é–“)
    serial_dict = defaultdict(int)          # æ¯å€‹è£½ç¨‹ç›®å‰ç·¨è™Ÿ
    key2serial = {}                         # (proc, m, start) âœ serial
    proc_serial_col = []

    for _, row in df.iterrows():
        key = (row['è£½ç¨‹åç¨±'], row['æ©Ÿå°'], row['é–‹å§‹(åˆ†)'])
        proc = row['è£½ç¨‹åç¨±']

        if key in key2serial:
            s = key2serial[key]             # åŒæ™‚æ®µåŒæ©Ÿå°å…±ç”¨ç·¨è™Ÿ
        else:
            serial_dict[proc] += 1
            s = serial_dict[proc]
            key2serial[key] = s
        proc_serial_col.append(s)

    df['Proc_Serial'] = proc_serial_col
    df['Batch_Label'] = df['è£½ç¨‹åç¨±'] + '-' + df['Proc_Serial'].astype(str)
    return df


# ===================================================================
# äºŒã€ç”˜ç‰¹åœ–
# ===================================================================
def plot_gantt(
    cur_df,
    prefix,
    save_prefix="ALL",
    legend_max_show=30,
    label_threshold=5   # æ©«æ¢å¯¬åº¦ < threshold (åˆ†é˜) å°±ä¸é¡¯ç¤ºç·¨è™Ÿ
):
    """
    ä¾ã€Œè£½ç¨‹åç¨±ã€æ±ºå®šé¡è‰²ï¼›åŒè£½ç¨‹åŒè‰²ã€‚
    æ¯æ®µæ©«æ¢ä¸­å¤®é¡¯ç¤ºè©²è£½ç¨‹çš„æµæ°´è™Ÿ (Proc_Serial)ã€‚
    è‹¥ w < label_threshold å‰‡ç•¥éæ–‡å­—ï¼Œä»¥å…æ“ åœ¨ä¸€èµ·ã€‚
    """
    # ---------- 1. åŸºæœ¬è¨­å®š ----------
    plt.rcParams["font.sans-serif"] = ["Microsoft JhengHei"]
    plt.rcParams["axes.unicode_minus"] = False
    kai = FontProperties(fname="C:/Windows/Fonts/msjh.ttc")

    df = cur_df.copy()
    if df.empty:
        print(f"âš ï¸ {prefix} ç„¡è³‡æ–™ï¼Œç”˜ç‰¹åœ–ç•¥éã€‚")
        return

    # y è»¸æ¨™ç±¤ï¼šä¸»é¡åˆ¥-æ©Ÿå°
    df["y_label"] = df["æ©Ÿå°ä¸»é¡åˆ¥"].astype(str) + "-" + df["æ©Ÿå°"].astype(str)
    order_df = (
        df[["æ©Ÿå°ä¸»é¡åˆ¥", "æ©Ÿå°"]]
        .drop_duplicates()
        .sort_values(["æ©Ÿå°ä¸»é¡åˆ¥", "æ©Ÿå°"])
    )
    ylabels = (order_df["æ©Ÿå°ä¸»é¡åˆ¥"] + "-" + order_df["æ©Ÿå°"].astype(str)).tolist()
    y2idx = {lbl: i for i, lbl in enumerate(ylabels)}

    # ---------- 2. é¡è‰²è¡¨ ----------
    proc_names = df["è£½ç¨‹åç¨±"].unique()
    cmap = get_cmap("tab20", len(proc_names))
    proc2color = {p: cmap(i) for i, p in enumerate(proc_names)}

    # ---------- 3. ç¹ªåœ– ----------
    fig, ax = plt.subplots(figsize=(14, max(6, len(ylabels) * 0.5)))

    for lbl, idx in y2idx.items():
        rows = df[df["y_label"] == lbl].sort_values("é–‹å§‹(åˆ†)")
        prev_end = 0
        for _, r in rows.iterrows():
            s, e = r["é–‹å§‹(åˆ†)"], r["çµæŸ(åˆ†)"]
            w = e - s

            # 3-1 ç©ºé–’æ®µ
            if s > prev_end:
                ax.broken_barh(
                    [(prev_end, s - prev_end)],
                    (idx - 0.3, 0.6),
                    facecolors="lightgrey",
                    alpha=0.7,
                    edgecolor="none",
                )

            # 3-2 å·¥ä½œæ®µ
            ax.broken_barh(
                [(s, w)],
                (idx - 0.3, 0.6),
                facecolors=proc2color[r["è£½ç¨‹åç¨±"]],
                edgecolor="black",
                alpha=0.9,
            )

            # 3-3 æ¨™æµæ°´è™Ÿ
            if w >= label_threshold:
                mid_x = s + w / 2
                ax.text(
                    mid_x,
                    idx,
                    str(r["Proc_Serial"]),       # åªé¡¯ç¤ºç·¨è™Ÿï¼›æ”¹æˆ r['Batch_Label'] ä¹Ÿå¯
                    ha="center",
                    va="center",
                    fontsize=6,
                    color="white",
                    weight="bold",
                )

            prev_end = e

    # ---------- 4. è»¸èˆ‡æ¨™é¡Œ ----------
    ax.set_yticks(range(len(ylabels)))
    ax.set_yticklabels(ylabels, fontproperties=kai, fontsize=12)
    ax.set_xlabel("æ™‚é–“ (åˆ†é˜)", fontproperties=kai, fontsize=14)
    ax.set_title(
        f"{prefix} æ©Ÿå°ç”˜ç‰¹åœ–ï¼ˆæŒ‰è£½ç¨‹è‘—è‰²ï¼‰",
        fontproperties=kai,
        fontsize=18,
    )
    ax.grid(True, axis="x", linestyle="--", alpha=0.7)

    # ---------- 5. åœ–ä¾‹ ----------
    show_procs = proc_names[:legend_max_show]
    handles = [Patch(facecolor=proc2color[p], label=p) for p in show_procs]
    handles.insert(0, Patch(facecolor="lightgrey", label="ç©ºé–’"))
    ax.legend(
        handles=handles,
        title="è£½ç¨‹åç¨±",
        loc="center left",
        bbox_to_anchor=(1.01, 0.5),
        fontsize=8,
        title_fontsize=10,
        frameon=True,
    )

    plt.tight_layout(rect=[0, 0, 0.75, 1])

    out_png = f"{save_prefix}_{prefix}_ProcColor_Gantt.png"
    plt.savefig(out_png, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"ğŸ¨ å·²è¼¸å‡ºç”˜ç‰¹åœ–ï¼š{out_png}")



# =============== ä¸»ç¨‹å¼åŸ·è¡Œ ===============
if __name__ == "__main__":
    merged_file = read_and_merge_data()
    sched_2A, sched_2B, mk = schedule_all(merged_file)

    # è£œä¸Š Batch_Label ä¾›ç”˜ç‰¹åœ–ä½¿ç”¨
    if not sched_2A.empty:
        sched_2A = add_proc_labels(sched_2A)
    if not sched_2B.empty:
        sched_2B = add_proc_labels(sched_2B)

    print("2A é ­å¹¾åˆ—ï¼š")
    print(sched_2A.head())
    print("2B é ­å¹¾åˆ—ï¼š")
    print(sched_2B.head())

    # ç¹ªè£½ç”˜ç‰¹åœ–
    if not sched_2A.empty:
        plot_gantt(sched_2A, '2A', save_prefix="GANTT")
    if not sched_2B.empty:
        plot_gantt(sched_2B, '2B', save_prefix="GANTT")

    print("ğŸ¯ ç”˜ç‰¹åœ–å·²å®Œæˆã€‚")
